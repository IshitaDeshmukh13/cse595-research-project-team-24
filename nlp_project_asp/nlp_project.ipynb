{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a974a5c3c4074e028835c9892de6e37b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3fc80e0869404d22895d7624726c5034",
              "IPY_MODEL_13c346109a684dfbb2c132713ec44e1a",
              "IPY_MODEL_d82ecf8d36144be198aa939eff10651d"
            ],
            "layout": "IPY_MODEL_9d468217c87b446281aab00ccd4bbb07"
          }
        },
        "3fc80e0869404d22895d7624726c5034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d714442bb08c4543a0d0ae4dd10439b3",
            "placeholder": "​",
            "style": "IPY_MODEL_a1da379a7dec4d9c945fb58a7fb887cb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "13c346109a684dfbb2c132713ec44e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7191d3ef572f4a34a409bb39855e73a3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69c2d8e5d5384ae2b5cebeb659f4f968",
            "value": 3
          }
        },
        "d82ecf8d36144be198aa939eff10651d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63c9daf99c7142249ba6d1b29132cc3c",
            "placeholder": "​",
            "style": "IPY_MODEL_66dcd6b0f41e45a5ad19a665611b6df9",
            "value": " 3/3 [00:05&lt;00:00,  1.84s/it]"
          }
        },
        "9d468217c87b446281aab00ccd4bbb07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d714442bb08c4543a0d0ae4dd10439b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1da379a7dec4d9c945fb58a7fb887cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7191d3ef572f4a34a409bb39855e73a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c2d8e5d5384ae2b5cebeb659f4f968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63c9daf99c7142249ba6d1b29132cc3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66dcd6b0f41e45a5ad19a665611b6df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UibvJsZxLzLF",
        "outputId": "b9f6429a-afe2-4a09-827e-40d51ef4f325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")\n",
        "!pip install transformers datasets\n",
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def generate_few_shots():\n",
        "    df = pd.read_csv('few_shot.csv').head(4)\n",
        "    df['group_1'] = df['group_1'].apply(eval)\n",
        "    df['group_2'] = df['group_2'].apply(eval)\n",
        "    return df"
      ],
      "metadata": {
        "id": "267iEQE3XfhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def load_trip(N):\n",
        "    test_dataset = load_dataset(\"sled-umich/TRIP\", split='ClozeTest')\n",
        "    df = pd.DataFrame(columns=['group_1', 'group_2', 'plausible'])\n",
        "    for i in range(N):\n",
        "        group_1 = test_dataset[i]['stories'][0]['sentences']\n",
        "        group_2 = test_dataset[i]['stories'][1]['sentences']\n",
        "        plausible_1 = test_dataset[i]['stories'][0]['plausible']\n",
        "        plausible = 1 if plausible_1 else 2\n",
        "        df.loc[i] = [group_1, group_2, plausible]\n",
        "    return df"
      ],
      "metadata": {
        "id": "fYnEM5cxL9Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "def get_zero_shot_prompt(group_1, group_2):\n",
        "    prompt = ''\n",
        "    for k, group in enumerate([group_1, group_2]):\n",
        "        prompt += f'Collection {k+1}:'\n",
        "        for index, sentence in enumerate(group):\n",
        "            prompt += '\\n' + str(index + 1)+ '. ' + sentence\n",
        "        prompt += '\\n'\n",
        "    prompt += '\\nGiven these two collections of sentences, choose which collection is more plausible. 1 or 2?'\n",
        "    prompt += '\\nProvide your answer in the format: The most plausible collection is <INSERT CHOSEN COLLECTION NUMBER HERE>.'\n",
        "    return prompt\n",
        "\n",
        "def get_few_shot_prompt(group_1, group_2, few_shots_df):\n",
        "    n_examples = 2\n",
        "    prompt = 'Given two collections of sentences, choose which collection is more plausible. 1 or 2?'\n",
        "    prompt += f'\\nUse these {n_examples} examples to guide your reasoning process.'\n",
        "    random = few_shots_df.sample(n=n_examples)\n",
        "    for k in range(n_examples):\n",
        "        prompt += '\\n\\nExample ' + str(k+1) + ':'\n",
        "        logical = random.iloc[k]['logical']\n",
        "        prompt += get_example_prompt(random.iloc[k]['group_1'], random.iloc[k]['group_2'], logical)\n",
        "    prompt += '\\n\\nNow, solve the following question.\\n'\n",
        "    for k, group in enumerate([group_1, group_2]):\n",
        "        prompt += 'Collection ' + str(k+1) + ':'\n",
        "        for index, sentence in enumerate(group):\n",
        "            prompt += '\\n' + str(index + 1)+ '. ' + sentence\n",
        "        prompt += '\\n'\n",
        "    prompt += 'Instructions: Given these two collections of sentences, choose which collection is more plausible. 1 or 2? '\n",
        "    prompt += 'Begin your answer with \"Analysis: \". Provide your chosen collection at the end of your answer in a json format: {\"result\": <INSERT CHOSEN COLLECTION NUMBER HERE>}.'\n",
        "    return prompt\n",
        "\n",
        "def get_example_prompt(group_1, group_2, logical):\n",
        "    prompt = '\\n'\n",
        "    for k, group in enumerate([group_1, group_2]):\n",
        "        prompt += f'Collection {k+1}:'\n",
        "        for index, sentence in enumerate(group):\n",
        "            prompt += '\\n' + str(index + 1)+ '. ' + sentence\n",
        "        prompt += '\\n'\n",
        "    prompt += 'Given these two collections of sentences, choose which collection is more plausible. 1 or 2?'\n",
        "    prompt += '\\n' + logical\n",
        "    return prompt\n",
        "\n",
        "def get_clingo_example_prompt(group_1, group_2, logical, clingo):\n",
        "    prompt = '\\n'\n",
        "    for k, group in enumerate([group_1, group_2]):\n",
        "        prompt += f'Collection {k+1}:'\n",
        "        for index, sentence in enumerate(group):\n",
        "            prompt += '\\n' + str(index + 1)+ '. ' + sentence\n",
        "        prompt += '\\n'\n",
        "    prompt += 'Given these two collections of sentences, model in ASP and choose which collection is more plausible. 1 or 2?'\n",
        "    prompt += '\\nThe ASP code is:\\n' + clingo\n",
        "    prompt += '\\n' + logical\n",
        "    return prompt\n",
        "\n",
        "def get_clingo_few_shot_prompt(group_1, group_2, few_shots_df):\n",
        "    n_examples = 2\n",
        "    prompt = 'Given two collections of sentences, you have 2 tasks.'\n",
        "    prompt += '\\n1) Model the sentences in Answer Set Programming.'\n",
        "    prompt += '\\n2) Choose which collection is more plausible. 1 or 2?'\n",
        "    prompt += f'\\nUse these {n_examples} examples to guide your reasoning process.'\n",
        "    random = few_shots_df.sample(n=n_examples)\n",
        "    for k in range(n_examples):\n",
        "        prompt += '\\n\\nExample ' + str(k+1) + ':'\n",
        "        logical = random.iloc[k]['logical']\n",
        "        clingo = random.iloc[k]['clingo']\n",
        "        prompt += get_clingo_example_prompt(random.iloc[k]['group_1'], random.iloc[k]['group_2'], logical, clingo)\n",
        "    prompt += '\\n\\nNow, solve the following question.\\n'\n",
        "    for k, group in enumerate([group_1, group_2]):\n",
        "        prompt += 'Collection ' + str(k+1) + ':'\n",
        "        for index, sentence in enumerate(group):\n",
        "            prompt += '\\n' + str(index + 1)+ '. ' + sentence\n",
        "        prompt += '\\n'\n",
        "    prompt += 'Instructions: Given these two collections of sentences, model the sentences in clingo and then choose which collection is more plausible. 1 or 2? '\n",
        "    prompt += 'Begin your answer with \"The ASP code is: \". Provide your chosen collection at the end of your answer in a json format: {\"result\": <INSERT CHOSEN COLLECTION NUMBER HERE>}.'\n",
        "    return prompt\n",
        "\n",
        "def extract_ground_truth(df):\n",
        "    ground_truth = []\n",
        "    for i in range(len(df)):\n",
        "        ground_truth.append(df.iloc[i]['plausible'])\n",
        "    return ground_truth\n",
        "\n",
        "def extract_answer(generated_text):\n",
        "    first_sentence = generated_text.split('.')[0]\n",
        "    if 'plausible' in first_sentence and 'collection' in first_sentence:\n",
        "        if '1' in first_sentence:\n",
        "            return 1\n",
        "        else:\n",
        "            return 2\n",
        "    return None\n",
        "\n",
        "def evaluate_predictions(predictions, ground_truth):\n",
        "    correct = sum(1 for p, g in zip(predictions, ground_truth) if p == g)\n",
        "    return correct / len(predictions) if predictions else 0\n",
        "\n",
        "def extract_json_result(text):\n",
        "    # print(\"JSON TEXT:\", text)\n",
        "    json_pattern = r'\\{[^{}]*\\}'\n",
        "    json_objects = re.findall(json_pattern, text)\n",
        "    for json_str in json_objects:\n",
        "        try:\n",
        "            parsed_json = json.loads(json_str)\n",
        "            if 'result' in parsed_json:\n",
        "                return parsed_json['result']\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    return None"
      ],
      "metadata": {
        "id": "yVTUhncfMiJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "def zero_shot_mistral(df, verbose=True):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\", max_length=600, device=device)\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(len(df)):\n",
        "        group_1 = df.iloc[i]['group_1']\n",
        "        group_2 = df.iloc[i]['group_2']\n",
        "        prompt = get_zero_shot_prompt(group_1, group_2)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert modeller in Answer Set Programming  and clingo.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "        generated_answer = chatbot(messages)[0]['generated_text'][-1]['content']\n",
        "        prediction = extract_answer(generated_answer)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"-------> Generated Answer\\n{generated_answer}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def few_shot_mistral(df, few_shot_df, verbose=True):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\", max_length=2000, device=device)\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(len(df)):\n",
        "        group_1 = df.iloc[i]['group_1']\n",
        "        group_2 = df.iloc[i]['group_2']\n",
        "        prompt = get_few_shot_prompt(group_1, group_2, few_shot_df)\n",
        "        # print('--------> Prompt')\n",
        "        # print(prompt)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert modeller in Answer Set Programming  and clingo.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "        generated_answer = chatbot(messages)[0]['generated_text'][-1]['content']\n",
        "        prediction = extract_json_result(generated_answer)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"-------> Generated Answer\\n{generated_answer}\")\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def clingo_few_shot_mistral(df, few_shot_df, verbose=True):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    chatbot = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\", max_length=3000, device=device)\n",
        "\n",
        "    predictions = []\n",
        "    answers = []\n",
        "    for i in range(len(df)):\n",
        "        group_1 = df.iloc[i]['group_1']\n",
        "        group_2 = df.iloc[i]['group_2']\n",
        "        prompt = get_clingo_few_shot_prompt(group_1, group_2, few_shot_df)\n",
        "        # print('--------> Prompt')\n",
        "        # print(prompt)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert modeller in Answer Set Programming  and clingo.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "        generated_answer = chatbot(messages)[0]['generated_text'][-1]['content']\n",
        "        answers.append(generated_answer)\n",
        "        prediction = extract_json_result(generated_answer)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"-------> Generated Answer\\n{generated_answer}\")\n",
        "\n",
        "    return predictions, answers\n"
      ],
      "metadata": {
        "id": "OFZSGCifMI5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_clingo_few_shot(N):\n",
        "    few_shot_df = generate_few_shots()\n",
        "    test_dataset = load_trip(N)\n",
        "    predictions, answers = clingo_few_shot_mistral(test_dataset, few_shot_df, verbose=False)\n",
        "    ground_truth = extract_ground_truth(test_dataset)\n",
        "    print(f\"Predictions: {predictions}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    accuracy = evaluate_predictions(predictions, ground_truth)\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    post_df = pd.DataFrame({'ground_truth': ground_truth, 'predictions': predictions, 'answers': answers})\n",
        "    return post_df\n",
        "\n",
        "post_df = main_clingo_few_shot(100)\n",
        "post_df.to_csv('clingo_few_shot_mistral_results.csv', index=False)\n",
        "# 100 samples. Accuracy = 62%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a974a5c3c4074e028835c9892de6e37b",
            "3fc80e0869404d22895d7624726c5034",
            "13c346109a684dfbb2c132713ec44e1a",
            "d82ecf8d36144be198aa939eff10651d",
            "9d468217c87b446281aab00ccd4bbb07",
            "d714442bb08c4543a0d0ae4dd10439b3",
            "a1da379a7dec4d9c945fb58a7fb887cb",
            "7191d3ef572f4a34a409bb39855e73a3",
            "69c2d8e5d5384ae2b5cebeb659f4f968",
            "63c9daf99c7142249ba6d1b29132cc3c",
            "66dcd6b0f41e45a5ad19a665611b6df9"
          ]
        },
        "id": "bOZ9WNSIz429",
        "outputId": "25920f14-8333-4ea5-d626-dce976d63f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a974a5c3c4074e028835c9892de6e37b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, '1, 2', 1, 1, 1, 1, 2, [1], 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1]\n",
            "Ground Truth: [2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2]\n",
            "Accuracy: 0.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def main_few_shot(N):\n",
        "#     few_shot_df = generate_few_shots()\n",
        "#     test_dataset = load_trip(N)\n",
        "#     predictions = few_shot_mistral(test_dataset, few_shot_df, verbose=False)\n",
        "#     ground_truth = extract_ground_truth(test_dataset)\n",
        "#     print(f\"Predictions: {predictions}\")\n",
        "#     print(f\"Ground Truth: {ground_truth}\")\n",
        "#     accuracy = evaluate_predictions(predictions, ground_truth)\n",
        "#     print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# main_few_shot(100)"
      ],
      "metadata": {
        "id": "EGZ8nyH05CLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main_clingo_few_shot(N):\n",
        "#     few_shot_df = generate_few_shots()\n",
        "#     test_dataset = load_trip(N)\n",
        "#     predictions = clingo_few_shot_mistral(test_dataset, few_shot_df, verbose=False)\n",
        "#     ground_truth = extract_ground_truth(test_dataset)\n",
        "#     print(f\"Predictions: {predictions}\")\n",
        "#     print(f\"Ground Truth: {ground_truth}\")\n",
        "#     accuracy = evaluate_predictions(predictions, ground_truth)\n",
        "#     print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# main_clingo_few_shot(100)"
      ],
      "metadata": {
        "id": "NTPZAv7xs1S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main_few_shot(N):\n",
        "#     few_shot_df = generate_few_shots()\n",
        "#     test_dataset = load_trip(N)\n",
        "#     predictions = few_shot_mistral(test_dataset, few_shot_df, verbose=False)\n",
        "#     ground_truth = extract_ground_truth(test_dataset)\n",
        "#     print(f\"Predictions: {predictions}\")\n",
        "#     print(f\"Ground Truth: {ground_truth}\")\n",
        "#     accuracy = evaluate_predictions(predictions, ground_truth)\n",
        "#     print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# main_few_shot(100)"
      ],
      "metadata": {
        "id": "pIpkBZ7gbSPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main(N):\n",
        "#     test_dataset = load_trip(N)\n",
        "#     # print_prompts(test_dataset)\n",
        "#     predictions = zero_shot_mistral(test_dataset, verbose=False)\n",
        "#     ground_truth = extract_ground_truth(test_dataset)\n",
        "#     print(f\"Predictions: {predictions}\")\n",
        "#     print(f\"Ground Truth: {ground_truth}\")\n",
        "#     accuracy = evaluate_predictions(predictions, ground_truth)\n",
        "#     # print_labels(test_dataset)\n",
        "#     print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# main(100)"
      ],
      "metadata": {
        "id": "m1EaA8phSY2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}